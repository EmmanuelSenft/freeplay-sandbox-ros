#!/usr/bin/env python
import numpy as np
import numpy.ma as ma
import math
import time, threading
import operator
import random

import rospy
from freeplay_sandbox_msgs.msg import DiscreteAction, ListFloatStamped 
from std_msgs.msg import String

DEBUG = False

class Actor(object):
    def __init__(self):
        self._state_sub = rospy.Subscriber("sparc/state", ListFloatStamped, self.on_state)
        self._trigger_state_sub = rospy.Subscriber("sparc/trigger_state", ListFloatStamped, self.on_trigger_state)
        self._action_sub = rospy.Subscriber("sparc/selected_action_dis",DiscreteAction, self.on_selected)
        self._action_sub = rospy.Subscriber("sparc/cancelled_action_dis",DiscreteAction, self.on_cancelled)
        self._event_sub = rospy.Subscriber("sparc/event", String, self.on_event)

        self._action_pub = rospy.Publisher("sparc/proposed_action_dis", DiscreteAction, queue_size = 5)
        self._actions = []
        self._trigger_states = []

        self._has_moved = False
        self._moving_Timer = threading.Timer(10, self.reset_has_moved)
        self._state = None
        self._proposed_action = None

    def on_state(self, message):
        self._state = np.array(message.data)
        ##print self._state

    def on_trigger_state(self, message):
        self._trigger_state = np.array(message.data)
#        print self._trigger_state
        if self._trigger_state[-3]%4 == 0:
            self.select_action()

    def on_selected(self, message):
        self.on_action(message, 1)

    def on_event(self, message):
        if message.data == "select":
            self.select_action()

    def on_cancelled(self, message):
        #print "cancelling"
        self.on_action(message,-1)

    def on_action(self, message, reward):
        #print "received action"
        action = message.action_id
        state_mask = np.array(message.state_mask,dtype = bool)
        masked_state = ma.array(self._state, mask = ~state_mask, dtype = float)
        if DEBUG:
            print "masked state"
            print masked_state
        for known_action in self._actions:
            if known_action._action == action:
                known_action._states.append(masked_state)
                known_action._rewards.append(reward)
                break
        else:
            new_action = Action(action, masked_state, reward)
            self._actions.append(new_action)
        trigger_mask = np.array(message.trigger_mask)
        masted_trigger_state = ma.array(self._trigger_state, mask = ~trigger_mask, dtype = float)
        self._trigger_states.append(masted_trigger_state)
        self._proposed_action = None
    
    # Similarity (euclidian distance normalised by the number of defined dimensions)
    # with x a masked state and y a normal one
    def get_similarity_states(self, x, y):
        if DEBUG:
            print x - y
        #similarity = 1 - math.sqrt(np.sum((x-y)*(x-y)))/sum(~x.mask)
        similarity = 1 - (np.sum((x-y)*(x-y)))/sum(~x.mask)
        if similarity is ma.masked:
            similarity = 0.5
        return similarity

    def select_action(self):
        #if DEBUG:
        print "selecting action"
        #print self._state
        if len(self._actions) == 0:
            return
        weighted_rewards = np.zeros(len(self._actions))
        indexes = np.zeros(len(self._actions),dtype=int)
        for i, action in enumerate(self._actions):
            print action._action
            reward = 0
            index = 0
            max_similarity = 0
            for idx, state in enumerate(action._states):
                print idx
                print "state " + str(state)
                print "reward " + str(action._rewards[idx])
                print "similarity " + str(self.get_similarity_states(state, self._state))
                similarity = self.get_similarity_states(state, self._state)
                if (similarity > max_similarity):
                    max_similarity = similarity
                    reward = action._rewards[idx]
                    index = idx
            weighted_rewards[i] = reward * max_similarity
            indexes[i] = index
        print "rewards"
        print weighted_rewards
        if max(weighted_rewards)<=0:
            print "no good candidate"
            return
        index_selected_action = np.argmax(weighted_rewards)
        proposed_action = self._actions[index_selected_action]

        print "index"
        print index_selected_action
        print "indexes"
        print indexes[index_selected_action]

        mask = ~self._actions[index_selected_action]._states[indexes[index_selected_action]].mask

        message = DiscreteAction()
        message.header.frame_id = "sandtray"
        message.header.stamp = rospy.Time(0)
        message.action_id = self._actions[index_selected_action]._action
#        for i in action.mask:
#            message.maskAction.append(bool(not i))
        message.state_mask=mask

        self._action_pub.publish(message)
#        self._has_moved = True
#        self._moving_Timer.start()

#        self._reaction_timer = threading.Timer(2, self.select_action)

    def reset_has_moved(self):
        self._has_moved = False
        self._moving_Timer = threading.Timer(10, self.reset_has_moved)

    def run(self):
        rospy.spin()

class Action(object):
    def __init__(self, act, state, reward):
        self._action = act
        self._states = [state]
        self._rewards = [reward]

if __name__ == "__main__":
    rospy.init_node('actor')
    actor = Actor()
    actor.run()
