#!/usr/bin/env python
import numpy as np
import numpy.ma as ma
import math
import time, threading
import operator
import random
import json

import rospy
from freeplay_sandbox_msgs.msg import DiscreteAction, ListFloatStamped 
from std_msgs.msg import String
import signal, sys
import os

DEBUG = False
DEFAULT_TRIGGER_NUMBER = 17

class Actor(object):
    def __init__(self):

        self._state_sub = rospy.Subscriber("sparc/state", ListFloatStamped, self.on_state)
        self._action_sub = rospy.Subscriber("sparc/selected_action_dis",DiscreteAction, self.on_action)
        self._event_sub = rospy.Subscriber("sparc/event", String, self.on_event)

        self._action_pub = rospy.Publisher("sparc/proposed_action_dis", DiscreteAction, queue_size = 5)
        self._trigger_number = rospy.get_param('~trigger_number', DEFAULT_TRIGGER_NUMBER)

        self._actions = []
        self._trigger_states = []
        self._waiting_states = []

        self._state = None
        self._threshold = .99

        self._use_files = rospy.get_param('~use_file',True)
        self._file_name = rospy.get_param('~file_name', os.path.expanduser('~') + "/Documents/foodchain-data/sparc/data.json")
        print "using files " + str(self._use_files)
        if self._use_files:
            try:
                self.load_json()
            except Exception:
                print "error while loading"
                pass

    def on_state(self, message):
        self._state = np.array(message.data)
        self.select_action()

    def check_trigger(self):
        if len(self._trigger_states)==0:
            return
        similarities_trigger = self.get_similarity_states(self._trigger_states,self._state[-self._trigger_number:])
        max_trigger = np.nanmax(similarities_trigger)
        similarities_waiting = self.get_similarity_states(self._waiting_states,self._state[-self._trigger_number:])
        max_waiting = np.nanmax(similarities_waiting)
        if not np.isnan(max_trigger) and max_trigger > .98 and max_trigger > max_waiting:
            self.select_action()

    def on_event(self, message):
        if message.data == "select":
            self.select_action()

    def on_action(self, message):
        action = message.action_id
        reward = message.reward
        state_mask = np.array(message.state_mask,dtype = bool)
        masked_state = ma.array(self._state, mask = ~state_mask, dtype = float)
        if DEBUG:
            print "masked state"
            print masked_state

        self.add_point(action, masked_state, reward)
    
    # Similarity (euclidian distance normalised by the number of defined dimensions)
    # with x a masked array and y a vector
    def get_similarity_states(self, x, y):
        similarity = np.array(1-np.divide(np.sum(np.square(x-y),axis=1),np.sum(~x.mask,axis=1)),dtype=np.float32)
        return similarity

    def select_action(self):
        if DEBUG:
            print "selecting action"
        if len(self._actions) == 0:
            return
        weighted_rewards = np.zeros(len(self._actions))
        indexes = np.zeros(len(self._actions),dtype=int)
        init = time.clock()
        for i, action in enumerate(self._actions):
            similarity = self.get_similarity_states(action._states,self._state)
            max_similarity = np.nanmax(similarity)
            if np.isnan(max_similarity):
                weighted_rewards[i] = -1
                indexes[i] = 0
            else:
                idx = len(similarity) - 1 - np.nanargmax(similarity[::-1])
                reward = action._rewards[idx]
                weighted_rewards[i] = reward * max_similarity
                indexes[i] = idx
        end = time.clock()
        print "Action selection"
        print "weighted reward " + str(weighted_rewards)
        print "max: " + str(np.nanmax(weighted_rewards))
        print "time: " + str(end-init)
        if np.nanmax(weighted_rewards)<=.95:
            print "no good candidate"
            return
        index_selected_action = np.nanargmax(weighted_rewards)
        proposed_action = self._actions[index_selected_action]

        if len(self._actions[index_selected_action]._states[indexes[index_selected_action]].mask.shape) == 0:
            mask = ~np.full(self._state.shape,self._actions[index_selected_action]._states[indexes[index_selected_action]].mask, dtype=bool)
        else:
            mask = ~self._actions[index_selected_action]._states[indexes[index_selected_action]].mask

        message = DiscreteAction()
        message.header.frame_id = "sandtray"
        message.header.stamp = rospy.Time(0)
        message.action_id = self._actions[index_selected_action]._action
        message.state_mask=mask
        message.reward = np.nanmax(weighted_rewards)

        self._action_pub.publish(message)

    def run(self):
        rospy.spin()

    def signal_handler(self, signal, frame):
        if self._use_files:
            self.save_json()
        sys.exit()

    def save_json(self):
        data=[]
        for a in self._actions:
            action = {"id": a._action}
            points = []
            for idx, reward in enumerate(a._rewards):
                item={"s":a._states[idx].tolist(),"r":reward}
                points.append(item)
            action["points"]=points
            data.append(action)
        threshold = {"threshold": self._threshold}
        data.append(threshold)
        with open(self._file_name,'w') as outfile:
            json.dump(data, outfile)

    def load_json(self):
        with open(self._file_name,'r') as infile:
            data=json.load(infile)
            for a in data:
                try:
                    id = a["id"]
                    points = a["points"]
                    for p in points:
                        m = (p["s"] == np.array(None))
                        d = np.where(p["s"] == np.array(None),0,p["s"])
                        s=ma.array(d, mask=m)
                        self.add_point(id, s, p["r"], True)
                except:
                    self._threshold = a["threshold"]

    def add_point(self, action, state, reward, loading = False):
        for known_action in self._actions:
            if known_action._action == action:
                if not loading:
                    previous_sim = max(self.get_similarity_states(known_action._states, self._state))
                    if previous_sim < self._threshold:
                        self._threshold -= ( self._threshold - previous_sim) / 5.
                reshaped_state=ma.reshape(state, (1,state.shape[0]))
                known_action._states=ma.append(known_action._states,reshaped_state,axis=0)
                known_action._rewards.append(reward)
                break
        else:
            new_action = Action(action, state, reward)
            self._actions.append(new_action)
        if reward > 0:
            if len(self._trigger_states) == 0:
                self._trigger_states=ma.reshape(state[-self._trigger_number:],(1,self._trigger_number))
            else:
                reshaped_trigger_states = ma.reshape(state[-self._trigger_number:],(1,self._trigger_number))
                self._trigger_states = ma.append(self._trigger_states,reshaped_trigger_states, axis=0) 

        # A reward of 0 indicate a waiting action
        if reward == 0:
            if len(self._waiting_states) == 0:
                self._waiting_states=ma.reshape(state[-self._trigger_number:],(1,self._trigger_number))
            else:
                reshaped_waiting_states = ma.reshape(state[-self._trigger_number:],(1,self._trigger_number))
                self._waiting_states = ma.append(self._waiting_states,reshaped_waiting_states, axis=0) 
            self._threshold += (1 - self._threshold) / 5. 

class Action(object):
    def __init__(self, act, state, reward):
        self._action = act
        self._states = ma.reshape(state,(1,state.shape[0]))
        self._rewards = [reward]

if __name__ == "__main__":
    rospy.init_node('actor')
    actor = Actor()
    signal.signal(signal.SIGINT, actor.signal_handler)
    actor.run()
