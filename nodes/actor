#!/usr/bin/env python
import numpy as np
import numpy.ma as ma
import math
import time, threading
import operator
import random
import json

import rospy
from freeplay_sandbox_msgs.msg import DiscreteAction, ListFloatStamped 
from std_msgs.msg import String
import signal, sys
import os

DEBUG = False
DEFAULT_TRIGGER_NUMBER = 17

class Actor(object):
    def __init__(self):

        self._state_sub = rospy.Subscriber("sparc/state", ListFloatStamped, self.on_state)
        self._action_sub = rospy.Subscriber("sparc/selected_action_dis",DiscreteAction, self.on_action)
        self._event_sub = rospy.Subscriber("sparc/event", String, self.on_event)

        self._action_pub = rospy.Publisher("sparc/proposed_action_dis", DiscreteAction, queue_size = 5)
        self._trigger_number = rospy.get_param('~trigger_number', DEFAULT_TRIGGER_NUMBER)

        self._actions = []
        self._trigger_states = []
        self._waiting_states = []

        self._state = None
        self._init_threshold = .5
        self._use_fixed_threshold = False
        self._threshold = self._init_threshold
        self._threshold_factor_decrease = 10
        self._threshold_factor_increase = 5

        self._neighbours_number = 1

        self._use_files = rospy.get_param('~use_file',True)
        self._file_name = rospy.get_param('~file_name', os.path.expanduser('~') + "/Documents/foodchain-data/sparc/data.json")
        print "using files " + str(self._use_files)
        if self._use_files:
            try:
                self.load_json()
            except Exception:
                print "error while loading"
                pass

    def on_state(self, message):
        self._state = np.array(message.data)
        self.select_action()

    def check_trigger(self):
        if len(self._trigger_states)==0:
            return
        similarities_trigger = self.get_similarity_states(self._trigger_states,self._state[-self._trigger_number:])
        max_trigger = np.nanmax(similarities_trigger)
        similarities_waiting = self.get_similarity_states(self._waiting_states,self._state[-self._trigger_number:])
        #max_waiting = np.nanmax(similarities_waiting)
        if not np.isnan(max_trigger) and max_trigger > .98:# and max_trigger > max_waiting:
            self.select_action()

    def on_event(self, message):
        if message.data == "select":
            self.select_action()
        if message.data == "reset":
            self.reset()
        if message.data.startswith("use_fixed_threshold"):
            self._use_fixed_threshold = bool(int(message.data.split("-")[1]))
        if message.data.startswith("init_threshold"):
            self._init_threshold = float(message.data.split("-")[1])
        if message.data.startswith("threshold_factor_increase"):
            self._threshold_factor_increase = float(message.data.split("-")[1])
        if message.data.startswith("threshold_factor_decrease"):
            self._threshold_factor_decrease = float(message.data.split("-")[1])
        if message.data.startswith("neighbours_number"):
            self._neighbours_number = float(message.data.split("-")[1])
    
    def reset(self):
        self._actions=[]
        self._threshold = self._init_threshold

    def on_action(self, message):
        if self._state is None:
            return
        action = message.action_id
        reward = message.reward
        state_mask = np.array(message.state_mask,dtype = bool)
        masked_state = ma.array(self._state, mask = ~state_mask, dtype = float)
        if reward<0:
            if DEBUG:
                print "----------------No-------------"
                print action
                print self._state
                print state_mask 

        self.add_point(action, masked_state, reward)
    
    # Similarity (euclidian distance normalised by the number of defined dimensions)
    # with x a masked array and y a vector
    def get_similarity_states(self, x, y):
        similarity = np.array(1-np.divide(np.sum(np.square(x-y),axis=1),np.sum(~x.mask,axis=1)),dtype=np.float32)
        return similarity

    def select_action(self):
        if DEBUG:
            print "selecting action"
        if len(self._actions) == 0:
            return
        weighted_rewards = np.zeros(len(self._actions))
        indexes = np.zeros(len(self._actions),dtype=int)
        for i, action in enumerate(self._actions):
            similarity = self.get_similarity_states(action._states,self._state)
            n = int(min(len(similarity), self._neighbours_number))
            idx_max = np.argpartition(similarity,-n)[-n:]
            idx_max = [j for j in idx_max if not np.isnan(similarity[j])]
            if len(idx_max)==0:
                weighted_rewards[i] = -1
                indexes[i] = 0
            else:
                try:
                    weighted_rewards[i] = np.median(action._rewards[idx_max] * similarity[idx_max])
                    indexes[i] = max(idx_max)
                except:
                    return

        if DEBUG:
            action_index = [a._action for a in self._actions]
            print "weighted reward " + str([x for _,x in sorted(zip(action_index, weighted_rewards))])
        if np.nanmax(weighted_rewards)<=self._threshold:
            return
        index_selected_action = np.nanargmax(weighted_rewards)
        proposed_action = self._actions[index_selected_action]

        if len(self._actions[index_selected_action]._states[indexes[index_selected_action]].mask.shape) == 0:
            mask = ~np.full(self._state.shape,self._actions[index_selected_action]._states[indexes[index_selected_action]].mask, dtype=bool)
        else:
            mask = ~self._actions[index_selected_action]._states[indexes[index_selected_action]].mask

        message = DiscreteAction()
        message.header.frame_id = "sandtray"
        message.header.stamp = rospy.Time(0)
        message.action_id = self._actions[index_selected_action]._action
        message.state_mask=mask
        message.reward = np.nanmax(weighted_rewards)

        self._action_pub.publish(message)

    def run(self):
        rospy.spin()

    def signal_handler(self, signal, frame):
        if self._use_files:
            self.save_json()
        sys.exit()

    def save_json(self):
        data=[]
        for a in self._actions:
            action = {"id": a._action}
            points = []
            for idx, reward in enumerate(a._rewards):
                item={"s":a._states[idx].tolist(),"r":reward}
                points.append(item)
            action["points"]=points
            data.append(action)
        threshold = {"threshold": self._threshold}
        data.append(threshold)
        with open(self._file_name,'w') as outfile:
            json.dump(data, outfile)

    def load_json(self):
        with open(self._file_name,'r') as infile:
            data=json.load(infile)
            for a in data:
                try:
                    id = a["id"]
                    points = a["points"]
                    for p in points:
                        m = (p["s"] == np.array(None))
                        d = np.where(p["s"] == np.array(None),0,p["s"])
                        s=ma.array(d, mask=m)
                        self.add_point(id, s, p["r"], True)
                except:
                    self._threshold = a["threshold"]

    def add_point(self, action, state, reward, loading = False):
        if DEBUG:
            print "Threshold "+str(self._threshold)
        for known_action in self._actions:
            if known_action._action == action:
                if not loading:
                    previous_sim = max(self.get_similarity_states(known_action._states, self._state))
                    if not self._use_fixed_threshold:
                        if reward > 0 and previous_sim < self._threshold:
                            self._threshold -= ( self._threshold - previous_sim) / self._threshold_factor_decrease
                        if reward <= 0 and previous_sim > self._threshold:
                            self._threshold += (previous_sim-self._threshold) / self._threshold_factor_increase
                reshaped_state=ma.reshape(state, (1,state.shape[0]))
                known_action._states=ma.masked_array(np.append(known_action._states.data,reshaped_state.data,axis=0),mask=np.append(known_action._states.mask,reshaped_state.mask,axis=0))
                known_action._rewards= np.append(known_action._rewards,reward)
                break
        else:
            new_action = Action(action, state, reward)
            self._actions.append(new_action)

        #Trigger state    
        if reward > 0:
            if len(self._trigger_states) == 0:
                self._trigger_states=ma.reshape(state[-self._trigger_number:],(1,self._trigger_number))
            else:
                reshaped_trigger_states = ma.reshape(state[-self._trigger_number:],(1,self._trigger_number))
                self._trigger_states = ma.append(self._trigger_states,reshaped_trigger_states, axis=0) 

        # A reward of 0 indicate a waiting action
        if reward == 0:
            if len(self._waiting_states) == 0:
                self._waiting_states=ma.reshape(state[-self._trigger_number:],(1,self._trigger_number))
            else:
                reshaped_waiting_states = ma.reshape(state[-self._trigger_number:],(1,self._trigger_number))
                self._waiting_states = ma.append(self._waiting_states,reshaped_waiting_states, axis=0) 

class Action(object):
    def __init__(self, act, state, reward):
        self._action = act
        self._states = ma.reshape(state,(1,state.shape[0]))
        self._rewards = np.array([reward])

if __name__ == "__main__":
    rospy.init_node('actor')
    actor = Actor()
    signal.signal(signal.SIGINT, actor.signal_handler)
    actor.run()
