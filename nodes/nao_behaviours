#!/usr/bin/env python
"""
Listens for a trajectory to write and sends it to the nao via naoqi SDK.

Requires a running robot/simulation with ALNetwork proxies.

"""
from copy import deepcopy
from naoqi import ALModule, ALBroker, ALProxy
from geometry_msgs.msg import PoseStamped
from nav_msgs.msg import Path
from std_msgs.msg import String
import rospy
import tf
import motion
import numpy
import math
import random
import threading

TO_RAD = math.pi / 180

# masks for which axes naoqi is to control with its planning
axisMask=motion.AXIS_MASK_VEL
#axisMask=motion.AXIS_MASK_ALL

HEAD_FRAME="Neck"

NAO_ARM_LENGTH=0.105 #m -> lenght of the link RBicep, from URDF
NAO_FOREARM_LENGTH=0.11 #m -> approx. lenght of forearm + half hand

# See
# http://doc.aldebaran.com/1-14/family/robots/joints_robot.html#robot-joints-v4-left-arm-joints
NAO_RSHOULDER_ANGLE_LIMITS = [-76 * TO_RAD, 18 * TO_RAD]
NAO_RELBOW_ANGLE_LIMITS = [2 * TO_RAD, 88.5 * TO_RAD]

tl = None

is_tracking_focus = False
robot_ready_to_point = False

last_pose = None

def clamp(v, limits):
    vmin, vmax = limits
    return max(min(v, vmax), vmin)


def xyz2pantilt(pose, headframe=HEAD_FRAME):
    """
    Convert a xyz target to pan and tilt angles for the head.

    :param headframe: the frame of the head
    :returns: (pan, tilt) in radians
    """
    #tl.waitForTransform(headframe,pose.header.frame_id, rospy.Time.now(), rospy.Duration(1.0))
    pose.header.stamp = tl.getLatestCommonTime(pose.header.frame_id, headframe)
    transformed_pose = tl.transformPose(headframe, pose)
    pan = numpy.arctan2(transformed_pose.pose.position.y, transformed_pose.pose.position.x)
    tilt = numpy.arctan2(transformed_pose.pose.position.z, transformed_pose.pose.position.x)
    
    return (pan,tilt)

def look_at(pose):
    yaw, pitch = xyz2pantilt(pose)
    yaw = clamp(yaw, (-2.0857, 2.0857))
    pitch = clamp(-pitch, (-0.6720, 0.5149)) #beware the '-pitch'!

    joints = ['HeadYaw','HeadPitch']
    angles = motionProxy.getAngles(joints, True)

    if abs(angles[0]-yaw) < 0.01 and abs(angles[1]-pitch) < 0.01:
       return

    #motionProxy.angleInterpolationWithSpeed(joints, [yaw, pitch], 0.1) # blocking
    speed = 0.1 if abs(angles[0] - yaw) > 0.1 else max(0.005, abs(angles[0]-yaw)) # adapt the speed to the distance to move
    rospy.logdebug("Moving head by: pan: %s, tilt: %s at %s%% of speed" % (yaw, pitch, speed * 100))
    print "x " +str(pose.pose.position.x) + " yaw "+str(yaw)
    motionProxy.setAngles(joints, [yaw, pitch], speed) # non-blocking


def arm_ik(targetpose, reference_frame="RShoulder"):
    """
    Returns (shoulder_joint, elbow_joint) such as the arm points towards the
    target (x,y). We make a simple 2D approximation.

              Y
     X target ^
      \  (x,y)|    O
       \      |     \l2
        \     |      \  elbow_joint
         \    |       O
          \   |      /
          `.  |     /l1
           |  |   ,'
            \ |  /
             \| / shoulder_joint
              O:............> X
    """

    targetpose.header.stamp = tl.getLatestCommonTime(targetpose.header.frame_id,reference_frame)
    pose = tl.transformPose(reference_frame, targetpose)

    # rotate the shoulder frame: on the robot, X points forward, on the drawing above, X points right
    x = -pose.pose.position.y
    y = pose.pose.position.x


    l1=NAO_ARM_LENGTH
    l2=NAO_FOREARM_LENGTH

    if x == 0 and y == 0:
        return None
    if y < 0: # do not allow gestures *behind* the robot
        return None

    if (math.sqrt(x*x + y*y) > l1+l2):
        shoulder_angle = -math.pi/2 + math.atan2(y,x)
        elbow_angle = 0.0
        if shoulder_angle > NAO_RSHOULDER_ANGLE_LIMITS[1]:
            elbow_angle = shoulder_angle

    else:
        # planar solution of the Inverse Kinematic problem
        elbow_angle = math.acos((x*x + y*y - l1*l1 - l2*l2)/(2*l1*l2))
    
        # note: -pi/2 comes from the orientation of Nao's joint (-> angle = 0 <=> arm pointing forward)
        shoulder_angle = -math.pi/2 + math.asin(y/math.sqrt(x*x + y*y)) - math.asin(l2 *
                                                math.sin(elbow_angle)/math.sqrt(x*x + y*y))
    

    return clamp(shoulder_angle, NAO_RSHOULDER_ANGLE_LIMITS),clamp(elbow_angle,NAO_RELBOW_ANGLE_LIMITS)



def point_at(targetpose):
    angles = arm_ik(targetpose)

    if angles is None:
        return

    names  = ["RShoulderRoll",
              "RElbowRoll"]
    rospy.logdebug("Moving shoulder, elbow: %s" % str([angles[0]/TO_RAD, angles[1]/TO_RAD]))
    fractionMaxSpeed  = 0.3
    motionProxy.angleInterpolationWithSpeed(names, angles, fractionMaxSpeed)
    pub_event.publish(String("pointing_finished"))

def set_pointing_posture():
    global robot_ready_to_point

    if not robot_ready_to_point:
        names  = ["RShoulderRoll",
                "RShoulderPitch", 
                "RElbowRoll", 
                "RElbowYaw",
                "RWristYaw"]
        angleLists  = [[-10 * TO_RAD, -75 * TO_RAD, -75 * TO_RAD, -35 * TO_RAD],
                    [70 * TO_RAD, 70 * TO_RAD, 10 * TO_RAD],
                    [2 * TO_RAD, 2 * TO_RAD, 60 * TO_RAD],
                    [0 * TO_RAD],
                    [0 * TO_RAD]]
        timeLists   = [[1.0, 3.0,4.0,5.0],
                    [1.0, 3.0, 4.0],
                    [1.0,4.0,5.0],
                    [1.0],
                    [1.0]]
        isAbsolute  = True
        motionProxy.angleInterpolation(names, angleLists, timeLists, isAbsolute) #blocking!

    robot_ready_to_point = True

def arm_to_rest_posture():
    global robot_ready_to_point

    if robot_ready_to_point:
        names  = ["RShoulderRoll",
                "RShoulderPitch", 
                "RElbowRoll"]
        angleLists  = [[-75 * TO_RAD, -75 * TO_RAD, -10 * TO_RAD],
                    [10 * TO_RAD, 70 * TO_RAD],
                    [2 * TO_RAD]]
        timeLists   = [[1.0, 3.0, 5.0],
                    [1.0, 3.0],
                    [1.5]]
        isAbsolute  = True
        motionProxy.angleInterpolation(names, angleLists, timeLists, isAbsolute) #blocking!

    robot_ready_to_point = False

def arm_between_pointing_posture():
    names  = ["RShoulderRoll",
              "RElbowRoll"]
    angleLists  = [[-random.randint(30,40) * TO_RAD],
                   [random.randint(50,70) * TO_RAD]]
    timeLists   = [[1.0],
    	       [1.0]]
    isAbsolute  = True
    motionProxy.angleInterpolation(names, angleLists, timeLists, isAbsolute) #blocking!

def on_pose(pose):
    global last_pose

    if is_tracking_focus:
        rospy.logwarn("Received a path, but Nao is going to ignore it as it currently track the ' robot focus'")
        return

    if(hasFallen == False): #no harm in executing trajectory
        #if(effector == "LArm"):
        #    motionProxy.openHand("LHand");
        #    roll = -1.7; #rotate wrist to the left (about the x axis, w.r.t. robot frame)
        #else:
        #    motionProxy.openHand("RHand");
        #    roll = 1.7; #rotate wrist to the right (about the x axis, w.r.t. robot frame)

        if not robot_ready_to_point:
            set_pointing_posture()
        else:
            threading.Thread(target=point_at,args=(pose,)).start()
            look_at(pose)

        last_pose = rospy.Time.now()


    else:
        rospy.loginfo("Got pose to look at, but I've fallen!");

def on_blocking_speech(sentence):
    ttsProxy.say(sentence.data)
    pub_event.publish(String("blocking_speech_finished"))
    


class FallResponder(ALModule):
  """ Module to react to robotHasFallen events """
  
  def __init__(self, name, motionProxy, memoryProxy):
      ALModule.__init__(self, name)
      self.motionProxy = motionProxy;
      memoryProxy.subscribeToEvent("robotHasFallen",name,self.has_fallen.__name__);
      rospy.loginfo("Subscribed to robotHasFallen event");
  def has_fallen(self, *_args):
      global hasFallen
      hasFallen = True;
      self.motionProxy.killAll();
      rospy.loginfo("Stopped task");
      
if __name__ == "__main__":

    rospy.init_node("nao_behaviours");
    
    POSES_TOPIC = rospy.get_param('~poses_output_topic','poses')
    NAO_IP = rospy.get_param('~nao_ip','127.0.0.1'); #default behaviour is 
                                        #to connect to simulator locally
    NAO_HANDEDNESS = rospy.get_param('~nao_handedness','right')
    if(NAO_HANDEDNESS.lower()=='right'):
        effector   = "RArm"
    elif(NAO_HANDEDNESS.lower()=='left'):
        effector = "LArm"
    else: 
        rospy.logerr('error in handedness param')

    pub_speech = rospy.Publisher("/speech", String, queue_size=5)
    
    sub_blocking_speech = rospy.Subscriber("/nao/blocking_speech", String, on_blocking_speech, queue_size=5)
    pub_event = rospy.Publisher("/nao/events", String, queue_size=5)


    # We need this broker to be able to construct
    # NAOqi modules and subscribe to other modules
    # The broker must stay alive until the program exists
    port = 9559;
    myBroker = ALBroker("myBroker", #I'm not sure that pyrobots doesn't already have one of these open called NAOqi?
        "0.0.0.0",   # listen to anyone
        0,           # find a free port and use it
        NAO_IP,      # parent broker IP
        port)        # parent broker port
    hasFallen = False;
    motionProxy = ALProxy("ALMotion", NAO_IP, port);
    memoryProxy = ALProxy("ALMemory", NAO_IP, port);
    ttsProxy = ALProxy("ALTextToSpeech", NAO_IP, port);
    postureProxy = ALProxy("ALRobotPosture", NAO_IP, port)
    #fallResponder = FallResponder("fallResponder",motionProxy,memoryProxy);
    
    motionProxy.wbEnableEffectorControl(effector,False); #if robot has fallen it will have a hard time getting up if the effector is still trying to be kept in a particular position
    motionProxy.wakeUp()
    postureProxy.goToPosture("StandInit", 0.5)

    set_pointing_posture()

    tl = tf.TransformListener()

    ### Check the robot is launched and properly localised wrt the sandtray
    while not rospy.is_shutdown():
        if not tl.frameExists("/odom"):
            rospy.logwarn("Waiting for frame /odom to be published...")
        else:
            try:
                t = tl.getLatestCommonTime("odom", "sandtray")
                if tl.canTransform("odom", "sandtray",t):
                    rospy.loginfo("Ok! Starting robot behaviours.")
                    pub_speech.publish(String("Ready to go!"))
                    break
                else:
                    rospy.logwarn("No transform robot->sandtray. Robot not yet localised!")
            except:
                rospy.logwarn("No transform robot->sandtray. Robot not yet localised!")

        rospy.sleep(0.5)

    sub_poses = rospy.Subscriber(POSES_TOPIC, PoseStamped, on_pose, queue_size=1) # queue_size = 1 to discard all but the most recent pose -- otherwise, tracking would possibly lag

    r = rospy.Rate(2)


    while not rospy.is_shutdown():
        try:
            t = tl.getLatestCommonTime(HEAD_FRAME, "robot_focus")
            if tl.canTransform(HEAD_FRAME, "robot_focus",t):
                is_tracking_focus = True
                robot_focus = PoseStamped()
                robot_focus.header.stamp = rospy.Time.now()
                robot_focus.header.frame_id = "robot_focus"
                if not robot_ready_to_point:
                    set_pointing_posture()
                else:
                    threading.Thread(target=point_at,args=(robot_focus,)).start()
                    look_at(robot_focus)
            else:
                is_tracking_focus = False
        except Exception as e: # most likely, robot_focus is not published. That's ok, don't track it then
            is_tracking_focus = False
            pass


        if last_pose and (rospy.Time.now() - last_pose).to_sec() > 5:
            if (rospy.Time.now() - last_pose).to_sec() < 30:
                arm_between_pointing_posture()
            else:
                arm_to_rest_posture()

        r.sleep()


    arm_to_rest_posture()
    motionProxy.rest()

    myBroker.shutdown()
